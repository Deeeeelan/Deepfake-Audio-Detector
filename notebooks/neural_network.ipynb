{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d75ded4",
   "metadata": {},
   "source": [
    "# Training Using Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f047ac1",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9131cb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Audio\n",
    "import soundfile as sf\n",
    "from multiprocessing import Pool\n",
    "import random\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torchvision\n",
    "from sklearn.metrics import classification_report\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab332b8c",
   "metadata": {},
   "source": [
    "Check if GPU acceleration is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6aa8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72174c78",
   "metadata": {},
   "source": [
    "### Load & Extract Data\n",
    "This time we will be extracting Mel-Spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaf56b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mfcc_extraction import make_spectrogram\n",
    "\n",
    "def multiprocess_spectrograms(paths):\n",
    "    shape = make_spectrogram(paths[0]).shape\n",
    "    spectrograms = np.empty((len(paths), shape[0], shape[1]), dtype=np.float32)\n",
    "    with Pool(os.cpu_count()) as p: \n",
    "        for i, spec in enumerate(p.imap(make_spectrogram, paths, chunksize=128)):\n",
    "            spectrograms[i] = spec\n",
    "            print(i)\n",
    "            if i % 1000 == 0:\n",
    "                print(f'{i}/{len(paths)} spectrograms computed')\n",
    "        print(f'{len(paths)}/{len(paths)} spectrograms computed') \n",
    "    return spectrograms\n",
    "\n",
    "def file_to_spec_tensor(path):\n",
    "    return torch.tensor(make_spectrogram(path), dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "def check_corrupted(path):\n",
    "    try:\n",
    "        sf.read(path, dtype=np.float32)\n",
    "        return True\n",
    "    except:\n",
    "        print(\"Corrupted:\", path)\n",
    "        return False\n",
    "\n",
    "def read_FoR_array(path):\n",
    "    fake_paths_training = []\n",
    "    real_paths_training = []\n",
    "\n",
    "    minus_fake = 0\n",
    "    minus_real = 0\n",
    "\n",
    "    for file in os.listdir(path + \"/fake\"):\n",
    "        curr_path = path + \"fake/\" + file\n",
    "        if file not in corrupted_files:\n",
    "            fake_paths_training.append(curr_path)\n",
    "        else:\n",
    "            minus_fake += 1\n",
    "    for file in os.listdir(path + \"/real\"):\n",
    "        curr_path = path + \"real/\" + file\n",
    "        real_paths_training.append(curr_path)\n",
    "    \n",
    "    paths = np.array(fake_paths_training + real_paths_training)\n",
    "    labels = np.array([0] * (len(fake_paths_training) - minus_fake) + [1] * (len(fake_paths_training) - minus_real))\n",
    "\n",
    "    return paths, labels\n",
    "\n",
    "class log_mel_spect_dataset(Dataset):\n",
    "    def __init__(self, spectrograms, labels):\n",
    "        self.spectrograms = torch.tensor(spectrograms, dtype=torch.float32).unsqueeze(1)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.spectrograms)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # feature_tensor = torch.tensor(self.spectrograms[idx], dtype=torch.float32).unsqueeze(0)\n",
    "        # label_tensor = torch.tensor(self.labels[idx], dtype=torch.float32).unsqueeze(0)\n",
    "        return self.spectrograms[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7223ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_protocol = '../data/ASVspoof_Dataset/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt'\n",
    "training_file_path = '../data/ASVspoof_Dataset/ASVspoof2019_LA_train/flac/'\n",
    "training_file_path2 = '../data/The Fake-or-Real (FoR) Dataset (deepfake audio)/training/'\n",
    "training_data_df = pd.read_csv(train_protocol, delimiter=\" \", names=[\"SPEAKER_ID\", \"AUDIO_FILE_NAME\", \"SYSTEM_ID\", \"-\", \"KEY\"])\n",
    "\n",
    "# The Fake-or-Real Dataset contains some corrupted files in the training set \n",
    "# which have been noted here to avoid\n",
    "corrupted_files = {\"file13424.mp3\",\"file15746.mp3\",\"file16643.mp3\",\"file17407.mp3\",\"file17450.mp3\",\"file19851.mp3\",\"file27206.mp3\",\"file27643.mp3\",\"file27839.mp3\",\"file30959.mp3\",\"file31017.mp3\",\"file32972.mp3\",\"file5323.mp3\",\"file9875.mp3\", \"file9904.mp3\"} \n",
    "\n",
    "\n",
    "test_protocol = '../data/ASVspoof_Dataset/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.eval.trl.txt'\n",
    "test_file_path = '../data/ASVspoof_Dataset/ASVspoof2019_LA_eval/flac/'\n",
    "test_file_path2 = '../data/The Fake-or-Real (FoR) Dataset (deepfake audio)/testing/'\n",
    "test_data_df = pd.read_csv(test_protocol, delimiter=\" \", names=[\"SPEAKER_ID\", \"AUDIO_FILE_NAME\", \"SYSTEM_ID\", \"-\", \"KEY\"])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "training_paths = training_file_path + training_data_df['AUDIO_FILE_NAME'].to_numpy() + \".flac\" \n",
    "training_labels = np.array(training_data_df['KEY'].map(lambda x: x == 'bonafide'))\n",
    "\"\"\"\n",
    "FoR_dataset_training = read_FoR_array(training_file_path2)\n",
    "training_paths = np.concatenate([training_paths, FoR_dataset_training[0]])\n",
    "training_labels = np.concatenate([training_labels, FoR_dataset_training[1]])\n",
    "\"\"\"\n",
    "\n",
    "print(training_paths[52223], training_paths[52222])\n",
    "\n",
    "test_paths = test_file_path + test_data_df['AUDIO_FILE_NAME'].to_numpy() + \".flac\" \n",
    "test_labels = np.array(test_data_df['KEY'].map(lambda x: x == 'bonafide'))\n",
    "\n",
    "\"\"\"\n",
    "FoR_Dataset_testing = read_FoR_array(test_file_path2)\n",
    "test_paths = np.concatenate([test_paths, FoR_Dataset_testing[0]])\n",
    "test_labels = np.concatenate([test_labels, FoR_Dataset_testing[1]])\n",
    "\"\"\"\n",
    "\n",
    "print(\"Computing Training Data:\")\n",
    "\n",
    "training_spectrograms = multiprocess_spectrograms(training_paths)\n",
    "training_dataset = log_mel_spect_dataset(training_spectrograms, training_labels)\n",
    "training_loader = DataLoader(training_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            num_workers=0,\n",
    "                            pin_memory=True,\n",
    "                            shuffle=True)\n",
    "\n",
    "print(\"Computing Test Data:\")\n",
    "test_spectrograms = multiprocess_spectrograms(test_paths)\n",
    "test_dataset = log_mel_spect_dataset(test_spectrograms, test_labels)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            num_workers=0,\n",
    "                            pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0904170a",
   "metadata": {},
   "source": [
    "### Build the Model\n",
    "Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e161bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 25\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=43648, out_features=600)\n",
    "        self.drop = nn.Dropout(0.25)\n",
    "        self.fc2 = nn.Linear(in_features=600, out_features=120)\n",
    "        self.fc3 = nn.Linear(in_features=120, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        out = torch.sigmoid(out) #TODO: use logits\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846bdb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net.to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(net.parameters(), \n",
    "                    lr=0.001,\n",
    "                    weight_decay=0.0001)\n",
    "\n",
    "\n",
    "loss_hist = []\n",
    "net.train()\n",
    "print(\"Beginning Training:\")\n",
    "for epoch in range(n_epoch):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(training_loader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    loss_hist.append(running_loss / len(training_loader))\n",
    "    print(f'Epoch: {epoch+1}, loss: {running_loss}')\n",
    "print('Finished Training')\n",
    "\n",
    "plt.plot(loss_hist)\n",
    "plt.title('Loss Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b280a42",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acba7b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './melspec1_net.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7ff84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949c36f4",
   "metadata": {},
   "source": [
    "### Evaluating Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4061ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH, weights_only=True))\n",
    "net.to(device)\n",
    "net.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6703d892",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "\n",
    "def predict(path):\n",
    "    outputs = net(file_to_spec_tensor(path).to(device).unsqueeze(0))\n",
    "    return (outputs >= threshold).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d594f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net(inputs)\n",
    "        predicted = (outputs >= threshold)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        predictions.extend(predicted.cpu().detach().numpy())\n",
    "\n",
    "print(f'Accuracy: {100 * correct // total} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199c6aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array(predictions)\n",
    "print(classification_report(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008a87f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(predict(\"../data/The Fake-or-Real (FoR) Dataset (deepfake audio)/training/file36.wav\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c23398",
   "metadata": {},
   "source": [
    "# Generating a Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c619c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = sklearn.metrics.confusion_matrix(test_labels, predictions)\n",
    "disp = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Spoof\", \"Bonafide\"])\n",
    "\n",
    "disp.plot()\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
